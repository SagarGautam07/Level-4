{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPChIiCu7Ya7"
      },
      "source": [
        "# Offline Chat-Reply Recommendation System (GPT-2)\n",
        "\n",
        "This notebook builds an offline, context-aware reply recommendation model for two-person chats:\n",
        "- Preprocess and tokenize long conversations efficiently\n",
        "- Fine-tune GPT-2 offline (local weights)\n",
        "- Generate coherent replies using User A history as context\n",
        "- Evaluate with BLEU/ROUGE/Perplexity\n",
        "- Save artifacts: `Model.joblib`, model dir, and guidance for `Report.pdf`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8OVVicD7Ya8",
        "outputId": "7f6a7c42-555e-4bf0-bdca-1302039deaf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports and setup\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2TokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure NLTK resources (no internet needed for BLEU)\n",
        "# BLEU does not require external downloads\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', DEVICE)\n",
        "set_seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDQ1Haq-7Ya9",
        "outputId": "594b85de-d555-46d0-8bbb-29112cc9c9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXCEL_PATH = ./conversationfile.xlsx - userAuserB.csv\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "class Config:\n",
        "    # Primary dataset is the Excel file placed next to this notebook\n",
        "    EXCEL_PATH = './conversationfile.xlsx - userAuserB.csv'  # adjust if your file lives elsewhere\n",
        "    if not os.path.exists(EXCEL_PATH):\n",
        "        # fallback: try parent directory\n",
        "        EXCEL_PATH = '../conversationfile.xlsx - userAuserB.csv'\n",
        "\n",
        "    # Model\n",
        "    MODEL_NAME = 'gpt2'  # small; local weights assumed available\n",
        "    MAX_SEQ_LEN = 512\n",
        "    MAX_CONTEXT_TURNS = 8  # number of turns from history to include\n",
        "\n",
        "    # Training\n",
        "    OUTPUT_DIR = './gpt2_chatrec_output'\n",
        "    NUM_EPOCHS = 1\n",
        "    BATCH_SIZE = 2\n",
        "    GRAD_ACC_STEPS = 8\n",
        "    LR = 5e-5\n",
        "    WARMUP_STEPS = 50\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    FP16 = torch.cuda.is_available()\n",
        "\n",
        "    # Generation\n",
        "    GEN_MAX_NEW_TOKENS = 64\n",
        "    TOP_K = 50\n",
        "    TOP_P = 0.95\n",
        "    TEMPERATURE = 0.8\n",
        "\n",
        "    # Splits\n",
        "    TRAIN_FRACTION = 0.9\n",
        "\n",
        "print('EXCEL_PATH =', Config.EXCEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RM4PCmNF7Ya9",
        "outputId": "9b7a44f4-4bb6-471d-d7c7-deaebc584900"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text speaker  conversation_id  \\\n",
              "0  \"Hey, did you see the client's feedback on the...       A                0   \n",
              "1  \"Just saw it. They want a lot of changes to th...       A                0   \n",
              "2  \"Yeah, that's what I was thinking. It's a big ...       A                0   \n",
              "3  \"I'll start on the revisions. Can you update t...       A                0   \n",
              "4  \"Will do. I'll block out the rest of the week ...       A                0   \n",
              "\n",
              "   turn_index  \n",
              "0           0  \n",
              "1           1  \n",
              "2           2  \n",
              "3           3  \n",
              "4           4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9ed8401-d1da-4ff1-a7c3-187d8aca4336\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>speaker</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>turn_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"Hey, did you see the client's feedback on the...</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Just saw it. They want a lot of changes to th...</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Yeah, that's what I was thinking. It's a big ...</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"I'll start on the revisions. Can you update t...</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Will do. I'll block out the rest of the week ...</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9ed8401-d1da-4ff1-a7c3-187d8aca4336')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9ed8401-d1da-4ff1-a7c3-187d8aca4336 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9ed8401-d1da-4ff1-a7c3-187d8aca4336');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1995d281-266d-4d80-b2aa-e63f8777bf87\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1995d281-266d-4d80-b2aa-e63f8777bf87')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1995d281-266d-4d80-b2aa-e63f8777bf87 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "conversations_df",
              "summary": "{\n  \"name\": \"conversations_df\",\n  \"rows\": 22,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"\\\"Hey, did you see the client's feedback on the mockups?\\\"\",\n          \"\\\"Tried it twice. Nothing.\\\"\",\n          \"\\\"Yeah, that's the one. Want to join?\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"conversation_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"turn_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 21,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Data loading from Excel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def load_conversations_from_excel(xlsx_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load an Excel workbook containing two-person chats and return a normalized\n",
        "    long-form dataframe with columns: conversation_id, turn_index, speaker, text.\n",
        "\n",
        "    Supported common schemas (auto-detected):\n",
        "    1) Long format: columns like [conversation_id, turn_index, speaker, text]\n",
        "    2) Wide per-row turns: columns like [A, B] or [userA, userB] representing alternating utterances\n",
        "    3) Single 'text' with 'speaker' columns, infers conversation_id if missing\n",
        "    \"\"\"\n",
        "    if not os.path.exists(xlsx_path):\n",
        "        raise FileNotFoundError(f\"File not found at {xlsx_path}. Please ensure the file is a CSV.\")\n",
        "\n",
        "    # Load as CSV\n",
        "    df = pd.read_csv(xlsx_path)\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    def to_long(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        # Case 1: already long\n",
        "        if {'conversation_id', 'turn_index', 'speaker', 'text'}.issubset(set(map(str.lower, df_in.columns))):\n",
        "            c = {c.lower(): c for c in df_in.columns}\n",
        "            out = pd.DataFrame()\n",
        "            out['conversation_id'] = df_in[c['conversation_id']]\n",
        "            out['turn_index'] = df_in[c['turn_index']]\n",
        "            out['speaker'] = df_in[c['speaker']].astype(str).str.strip().str.upper().str[0]\n",
        "            out['text'] = df_in[c['text']].astype(str)\n",
        "            return out\n",
        "\n",
        "        # Case 2: wide A/B columns\n",
        "        a_col = cols_lower.get('a') or cols_lower.get('usera') or cols_lower.get('speaker_a')\n",
        "        b_col = cols_lower.get('b') or cols_lower.get('userb') or cols_lower.get('speaker_b')\n",
        "        if a_col and b_col:\n",
        "            records = []\n",
        "            conv_id = 0\n",
        "            for _, row in df_in.iterrows():\n",
        "                a_text = str(row[a_col]) if not pd.isna(row[a_col]) else None\n",
        "                b_text = str(row[b_col]) if not pd.isna(row[b_col]) else None\n",
        "                turn_idx = 0\n",
        "                if a_text:\n",
        "                    records.append({'conversation_id': conv_id, 'turn_index': turn_idx, 'speaker': 'A', 'text': a_text})\n",
        "                    turn_idx += 1\n",
        "                if b_text:\n",
        "                    records.append({'conversation_id': conv_id, 'turn_index': turn_idx, 'speaker': 'B', 'text': b_text})\n",
        "                conv_id += 1\n",
        "            return pd.DataFrame.from_records(records)\n",
        "\n",
        "        # Case 3: generic text + speaker\n",
        "        text_c = cols_lower.get('text') or cols_lower.get('message') or list(df_in.columns)[-1]\n",
        "        speaker_c = cols_lower.get('speaker') or None\n",
        "        out = pd.DataFrame()\n",
        "        out['text'] = df_in[text_c].astype(str)\n",
        "        out['speaker'] = (df_in[speaker_c].astype(str) if speaker_c else 'A')\n",
        "        out['speaker'] = out['speaker'].str.strip().str.upper().str[0]\n",
        "        out['conversation_id'] = 0\n",
        "        out['turn_index'] = out.groupby('conversation_id').cumcount()\n",
        "        return out\n",
        "\n",
        "    long_df = to_long(df)\n",
        "    long_df = long_df.sort_values(by=['conversation_id', 'turn_index']).reset_index(drop=True)\n",
        "    return long_df\n",
        "\n",
        "conversations_df = load_conversations_from_excel(Config.EXCEL_PATH)\n",
        "conversations_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxXuFvMH7Ya-",
        "outputId": "a605e601-98f6-4aba-c146-f8920645dab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in dataframe: 22\n",
            "  Processing conversation 0 with 22 turns.\n",
            "Finished building examples. Total examples created: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, None)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Preprocessing: build (context -> next A reply) training examples\n",
        "\n",
        "def build_examples(df: pd.DataFrame, max_context_turns: int) -> List[Dict[str, Any]]:\n",
        "    examples: List[Dict[str, Any]] = []\n",
        "    print(f\"Total rows in dataframe: {len(df)}\")\n",
        "    for conv_id, group in df.groupby('conversation_id'):\n",
        "        turns = group[['speaker', 'text']].values.tolist()\n",
        "        print(f\"  Processing conversation {conv_id} with {len(turns)} turns.\")\n",
        "        # We predict the next A reply when the last observed speaker is B\n",
        "        for idx in range(len(turns)):\n",
        "            # find indices where turns[idx] is B and there exists a following A\n",
        "            if turns[idx][0] != 'B':\n",
        "                # print(f\"    Turn {idx}: Speaker is not B ({turns[idx][0]}), skipping.\")\n",
        "                continue\n",
        "            # find next A after idx\n",
        "            next_a_idx = None\n",
        "            for j in range(idx + 1, len(turns)):\n",
        "                if turns[j][0] == 'A':\n",
        "                    next_a_idx = j\n",
        "                    break\n",
        "            if next_a_idx is None:\n",
        "                # print(f\"    Turn {idx}: Speaker is B, but no following A found, skipping.\")\n",
        "                continue\n",
        "            start = max(0, idx - max_context_turns + 1)\n",
        "            context_turns = turns[start:idx + 1]  # include B message\n",
        "            target_a = turns[next_a_idx][1]\n",
        "            examples.append({\n",
        "                'conversation_id': conv_id,\n",
        "                'context': context_turns,  # list of [speaker, text]\n",
        "                'target': target_a,\n",
        "            })\n",
        "            # print(f\"    Turn {idx}: Found example. Context length: {len(context_turns)}, Target: {target_a[:50]}...\")\n",
        "\n",
        "    print(f\"Finished building examples. Total examples created: {len(examples)}\")\n",
        "    return examples\n",
        "\n",
        "examples = build_examples(conversations_df, Config.MAX_CONTEXT_TURNS)\n",
        "len(examples), examples[0] if examples else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1TrKces7Ya-",
        "outputId": "915b4a29-d1d7-4f75-9159-defc7f4a3a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added special tokens: 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Tokenizer with speaker tokens\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(Config.MODEL_NAME)\n",
        "SPECIAL_TOKENS = {\n",
        "    'bos_token': '<bos>',\n",
        "    'eos_token': '<eos>',\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<A>', '<B>', '<SEP>']\n",
        "}\n",
        "\n",
        "# Add tokens if missing\n",
        "num_added = tokenizer.add_special_tokens({k: v for k, v in SPECIAL_TOKENS.items() if k != 'additional_special_tokens'})\n",
        "num_added += tokenizer.add_special_tokens({'additional_special_tokens': SPECIAL_TOKENS['additional_special_tokens']})\n",
        "print('Added special tokens:', num_added)\n",
        "\n",
        "# Helper to linearize context\n",
        "\n",
        "def render_example(ex: Dict[str, Any]) -> str:\n",
        "    parts: List[str] = []\n",
        "    for speaker, text in ex['context']:\n",
        "        marker = '<A>' if speaker == 'A' else '<B>'\n",
        "        parts.append(f\"{marker} {text} <SEP>\")\n",
        "    parts.append('<A>')  # indicate we want A to continue\n",
        "    return ' '.join(parts)\n",
        "\n",
        "# Train/Val split\n",
        "random.shuffle(examples)\n",
        "train_size = int(len(examples) * Config.TRAIN_FRACTION)\n",
        "train_examples = examples[:train_size]\n",
        "val_examples = examples[train_size:]\n",
        "len(train_examples), len(val_examples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DSWZCBz7Ya-",
        "outputId": "bd6e71b5-708f-4184-ea91-513df737ab15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Dataset and collator\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, examples: List[Dict[str, Any]], tokenizer: GPT2TokenizerFast, max_length: int):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        ex = self.examples[idx]\n",
        "        prompt = render_example(ex)\n",
        "        # Input is prompt + target + eos\n",
        "        full_text = f\"{prompt} {ex['target']} <eos>\"\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "        # Labels equal input_ids for causal LM; set padding to -100\n",
        "        labels = input_ids.clone()\n",
        "        labels[attention_mask == 0] = -100\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "        }\n",
        "\n",
        "train_ds = ChatDataset(train_examples, tokenizer, Config.MAX_SEQ_LEN)\n",
        "val_ds = ChatDataset(val_examples, tokenizer, Config.MAX_SEQ_LEN)\n",
        "len(train_ds), len(val_ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVWvEOIz7Ya-",
        "outputId": "42990d1c-686a-489a-9652-567c4e4008f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer ready\n"
          ]
        }
      ],
      "source": [
        "# Model and Trainer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(Config.MODEL_NAME)\n",
        "# Resize embeddings to account for new special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "# Ensure pad token id is set on config to avoid warnings during generation\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.to(DEVICE)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=Config.OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=Config.NUM_EPOCHS,\n",
        "    per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=Config.BATCH_SIZE,\n",
        "    gradient_accumulation_steps=Config.GRAD_ACC_STEPS,\n",
        "    # Removed evaluation_strategy and eval_steps for now\n",
        "    save_steps=200,\n",
        "    logging_steps=50,\n",
        "    learning_rate=Config.LR,\n",
        "    warmup_steps=Config.WARMUP_STEPS,\n",
        "    weight_decay=Config.WEIGHT_DECAY,\n",
        "    fp16=Config.FP16,\n",
        "    report_to=[],  # disable wandb\n",
        ")\n",
        "\n",
        "# Standard LM collator; we already set labels, so no mlm\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    # eval_dataset=val_ds, # Removed eval_dataset as evaluation is disabled\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print('Trainer ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "hmxae0Nn7Ya_",
        "outputId": "0c234436-4862-4461-b9dc-f7177e39f1c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-509286330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train (offline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer: training requires a train_dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         return self._get_dataloader(\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_dataloader\u001b[0;34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampler_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m                 \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sampler\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"drop_last\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_drop_last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mdataloader_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prefetch_factor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_prefetch_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_train_sampler\u001b[0;34m(self, train_dataset)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m     def _get_dataloader(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "# Train (offline)\n",
        "\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics\n",
        "trainer.save_model(Config.OUTPUT_DIR)\n",
        "trainer.save_state()\n",
        "\n",
        "print('Train metrics:', metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5O7kajR57Ya_"
      },
      "outputs": [],
      "source": [
        "# Generation utility\n",
        "\n",
        "def generate_reply(context_turns: List[Tuple[str, str]],\n",
        "                   max_new_tokens: int = Config.GEN_MAX_NEW_TOKENS) -> str:\n",
        "    prompt = ' '.join([(('<A>' if s=='A' else '<B>') + ' ' + t + ' <SEP>') for s, t in context_turns])\n",
        "    prompt += ' <A>'\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_k=Config.TOP_K,\n",
        "            top_p=Config.TOP_P,\n",
        "            temperature=Config.TEMPERATURE,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    gen = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    # Extract only the continuation after the last '<A>' marker\n",
        "    last_a = gen.rfind('<A>')\n",
        "    continuation = gen[last_a+3:] if last_a >= 0 else gen\n",
        "    # stop at first special token\n",
        "    for stop_tok in ['<eos>', '<SEP>', '<A>', '<B>']:\n",
        "        idx = continuation.find(stop_tok)\n",
        "        if idx != -1:\n",
        "            continuation = continuation[:idx]\n",
        "    return continuation.strip()\n",
        "\n",
        "# Quick smoke test (if validation exists)\n",
        "if len(val_examples) > 0:\n",
        "    sample = val_examples[0]\n",
        "    print('Context:', sample['context'])\n",
        "    print('Target:', sample['target'])\n",
        "    print('Generated:', generate_reply(sample['context']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Y_D6bZ7Ya_",
        "outputId": "b14917e9-4751-4bb5-d00b-01bc5587c1c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BLEU': 0.0,\n",
              " 'ROUGE-1_F1': 0.0,\n",
              " 'ROUGE-2_F1': 0.0,\n",
              " 'ROUGE-L_F1': 0.0,\n",
              " 'Perplexity': inf}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# Evaluation: BLEU, ROUGE, Perplexity\n",
        "\n",
        "smooth = SmoothingFunction().method3\n",
        "\n",
        "\n",
        "def compute_bleu(reference: str, hypothesis: str) -> float:\n",
        "    ref_tokens = reference.split()\n",
        "    hyp_tokens = hypothesis.split()\n",
        "    return sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth)\n",
        "\n",
        "\n",
        "def rouge_n(reference: str, hypothesis: str, n: int = 1) -> Tuple[float, float, float]:\n",
        "    # Simple ROUGE-N (recall, precision, F1) using token n-grams\n",
        "    def ngrams(tokens: List[str], n: int):\n",
        "        return list(zip(*[tokens[i:] for i in range(n)]))\n",
        "    ref = reference.split()\n",
        "    hyp = hypothesis.split()\n",
        "    ref_ngrams = ngrams(ref, n)\n",
        "    hyp_ngrams = ngrams(hyp, n)\n",
        "    ref_counts = {}\n",
        "    for g in ref_ngrams:\n",
        "        ref_counts[g] = ref_counts.get(g, 0) + 1\n",
        "    overlap = 0\n",
        "    hyp_counts = {}\n",
        "    for g in hyp_ngrams:\n",
        "        hyp_counts[g] = hyp_counts.get(g, 0) + 1\n",
        "    for g, c in hyp_counts.items():\n",
        "        overlap += min(c, ref_counts.get(g, 0))\n",
        "    recall = overlap / max(1, len(ref_ngrams))\n",
        "    precision = overlap / max(1, len(hyp_ngrams))\n",
        "    f1 = 0.0 if (recall + precision) == 0 else 2 * recall * precision / (recall + precision)\n",
        "    return recall, precision, f1\n",
        "\n",
        "\n",
        "def rouge_l(reference: str, hypothesis: str) -> Tuple[float, float, float]:\n",
        "    # ROUGE-L based on LCS\n",
        "    ref = reference.split()\n",
        "    hyp = hypothesis.split()\n",
        "    dp = [[0]*(len(hyp)+1) for _ in range(len(ref)+1)]\n",
        "    for i in range(1, len(ref)+1):\n",
        "        for j in range(1, len(hyp)+1):\n",
        "            if ref[i-1] == hyp[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "    lcs = dp[-1][-1]\n",
        "    recall = lcs / max(1, len(ref))\n",
        "    precision = lcs / max(1, len(hyp))\n",
        "    f1 = 0.0 if (recall + precision) == 0 else 2 * recall * precision / (recall + precision)\n",
        "    return recall, precision, f1\n",
        "\n",
        "\n",
        "def evaluate_dataset(eval_examples: List[Dict[str, Any]], num_samples: int = 100) -> Dict[str, float]:\n",
        "    sample_examples = eval_examples[:max(1, min(num_samples, len(eval_examples)))]\n",
        "    bleu_scores = []\n",
        "    r1_f1 = []\n",
        "    r2_f1 = []\n",
        "    rl_f1 = []\n",
        "    for ex in sample_examples:\n",
        "        hyp = generate_reply(ex['context'])\n",
        "        ref = ex['target']\n",
        "        bleu_scores.append(compute_bleu(ref, hyp))\n",
        "        r1_f1.append(rouge_n(ref, hyp, n=1)[2])\n",
        "        r2_f1.append(rouge_n(ref, hyp, n=2)[2])\n",
        "        rl_f1.append(rouge_l(ref, hyp)[2])\n",
        "    ppl = math.exp(trainer.evaluate()['eval_loss']) if len(sample_examples) > 0 else float('inf')\n",
        "    return {\n",
        "        'BLEU': float(np.mean(bleu_scores)) if bleu_scores else 0.0,\n",
        "        'ROUGE-1_F1': float(np.mean(r1_f1)) if r1_f1 else 0.0,\n",
        "        'ROUGE-2_F1': float(np.mean(r2_f1)) if r2_f1 else 0.0,\n",
        "        'ROUGE-L_F1': float(np.mean(rl_f1)) if rl_f1 else 0.0,\n",
        "        'Perplexity': float(ppl),\n",
        "    }\n",
        "\n",
        "metrics_eval = evaluate_dataset(val_examples, num_samples=100)\n",
        "metrics_eval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6QX6qhb7Ya_",
        "outputId": "321f5bf3-e8c2-46bc-e06b-97e444cd1d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts saved to ./gpt2_chatrec_output and Model.joblib\n"
          ]
        }
      ],
      "source": [
        "# Save artifacts: Model.joblib and tokenizer/model dirs\n",
        "\n",
        "class InferenceModel:\n",
        "    def __init__(self, model_dir: str, tokenizer_dir: str):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_dir)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_dir).to(self.device)\n",
        "\n",
        "    def reply(self, context_turns: List[Tuple[str, str]],\n",
        "              max_new_tokens: int = Config.GEN_MAX_NEW_TOKENS) -> str:\n",
        "        prompt = ' '.join([(('<A>' if s=='A' else '<B>') + ' ' + t + ' <SEP>') for s, t in context_turns])\n",
        "        prompt += ' <A>'\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                top_k=Config.TOP_K,\n",
        "                top_p=Config.TOP_P,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "        gen = self.tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "        last_a = gen.rfind('<A>')\n",
        "        continuation = gen[last_a+3:] if last_a >= 0 else gen\n",
        "        for stop_tok in ['<eos>', '<SEP>', '<A>', '<B>']:\n",
        "            idx = continuation.find(stop_tok)\n",
        "            if idx != -1:\n",
        "                continuation = continuation[:idx]\n",
        "        return continuation.strip()\n",
        "\n",
        "# Save tokenizer and model directories\n",
        "model.save_pretrained(Config.OUTPUT_DIR, safe_serialization=False) # Added safe_serialization=False\n",
        "tokenizer.save_pretrained(Config.OUTPUT_DIR)\n",
        "\n",
        "# Save a lightweight joblib wrapper\n",
        "wrapper = InferenceModel(Config.OUTPUT_DIR, Config.OUTPUT_DIR)\n",
        "joblib.dump(wrapper, 'Model.joblib')\n",
        "\n",
        "print('Artifacts saved to', Config.OUTPUT_DIR, 'and Model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDWzuVPR7Ya_"
      },
      "source": [
        "## Report Guidance (for Report.pdf)\n",
        "\n",
        "Include the following in your short report:\n",
        "- Data description and preprocessing choices (context window, cleaning)\n",
        "- Model choice rationale (GPT-2 small, causal LM for next-reply)\n",
        "- Training setup (epochs, batch size, LR, hardware)\n",
        "- Offline feasibility (no internet, local weights)\n",
        "- Evaluation results (BLEU/ROUGE/Perplexity) with a brief interpretation\n",
        "- Error analysis and example generations\n",
        "- Deployment notes (latency on CPU/GPU, memory footprint, batching)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}